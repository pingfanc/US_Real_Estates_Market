{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4983f36-beea-4129-8941-98a8b40c3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "host=\"13.43.84.151\"\n",
    "dbname=\"postgres\"\n",
    "user=\"ping-fan.chen.23@ucl.ac.uk\"\n",
    "password=\"khsLcR\"\n",
    "port=\"5432\"\n",
    "schema=\"schema_pingfanchen23uclacuk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc2d115-f204-4d87-a9ca-c9d83d4e3eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect (\n",
    "    host=host,\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port=port\n",
    ")\n",
    "print(\"Database connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724503c5-c025-4e63-8a5d-4bfd83cb9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1015k  100 1015k    0     0   642k      0  0:00:01  0:00:01 --:--:--  642k\n"
     ]
    }
   ],
   "source": [
    "# fetch the java driver for postgress to allow Spark to connect to postgress\n",
    "!curl -o postgresql-42.3.2.jar https://jdbc.postgresql.org/download/postgresql-42.3.2.jar\n",
    "# uses the curl command to download the PostgreSQL JDBC driver from the official site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0308bb-3708-4453-b068-c1bb6fc54bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/dataengineer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/dataengineer/postgresql-42.3.2.jar'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd() # assigns the current working directory to the variable current_dir\n",
    "print(current_dir)\n",
    "\n",
    "jar_location = current_dir + \"/postgresql-42.3.2.jar\"\n",
    "# Constructs the file path for the PostgreSQL JDBC driver JAR file by appending the filename to the current directory path\n",
    "jar_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0c333c2-87ec-4436-8b57-fd80b80388cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/08 17:26:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraClassPath\", jar_location)\\\n",
    "    .appName(\"Enhanced Data Warehouse ETL\") \\\n",
    "    .getOrCreate()\n",
    "# Sets the configuration property spark.driver.extraClassPath to the path of the JDBC driver.\n",
    "# This tells Spark where to find the driver for the database connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec695dc-17dc-466f-93a9-4c42fef8a819",
   "metadata": {},
   "source": [
    "### ZIP Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e027e3d-00ac-4f5d-8366-72d9815d26ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, lit, lead\n",
    "\n",
    "# using api\n",
    "import requests\n",
    "\n",
    "states = ['AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL','IN','IA',\n",
    "        'KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM',\n",
    "        'NY','NC','ND','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV',\n",
    "        'WI','WY']\n",
    "city = \"\"\n",
    "states_zip = {}\n",
    "for state in states:\n",
    "    api_url = 'https://api.api-ninjas.com/v1/zipcode?city={}&state={}'.format(city, state)\n",
    "\n",
    "    response = requests.get(api_url+city, headers={'X-Api-Key': '0+WwzqaBAr5kWjywXArIpA==DlpdDtBfCV9Pb7Nw'})\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        data = response.json()\n",
    "        df = spark.createDataFrame(data)\n",
    "        states_zip[state] = df\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "df_all = reduce(DataFrame.unionAll, states_zip.values())\n",
    "# Show the dataframe\n",
    "df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1abb4fd-e902-44c0-a513-d3f6ca8350a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------+-----------------+-------+--------+-----+---------------+-----+--------+\n",
      "|area_codes|          city|country|           county|    lat|     lon|state|       timezone|valid|zip_code|\n",
      "+----------+--------------+-------+-----------------+-------+--------+-----+---------------+-----+--------+\n",
      "|     [205]|         Moody|     US| St. Clair County|33.6034|-86.4944|   AL|America/Chicago| true|   35004|\n",
      "|     [205]|    Adamsville|     US| Jefferson County|33.5929|-86.9940|   AL|America/Chicago| true|   35005|\n",
      "|     [205]|         Adger|     US| Jefferson County|33.4462|-87.2230|   AL|America/Chicago| true|   35006|\n",
      "|     [205]|     Alabaster|     US|    Shelby County|33.2187|-86.7835|   AL|America/Chicago| true|   35007|\n",
      "|     [256]|Alexander City|     US|Tallapoosa County|32.9011|-85.9178|   AL|America/Chicago| true|   35010|\n",
      "|     [256]|Alexander City|     US|Tallapoosa County|32.9394|-85.9466|   AL|America/Chicago| true|   35011|\n",
      "|     [205]|       Allgood|     US|    Blount County|33.9119|-86.5094|   AL|America/Chicago| true|   35013|\n",
      "|     [256]|        Alpine|     US| Talladega County|33.3452|-86.2657|   AL|America/Chicago| true|   35014|\n",
      "|     [205]|         Alton|     US| Jefferson County|33.5795|-86.6376|   AL|America/Chicago| true|   35015|\n",
      "|     [256]|          Arab|     US|  Marshall County|34.3218|-86.4969|   AL|America/Chicago| true|   35016|\n",
      "|        []|     Baileyton|     US|   Cullman County|34.3045|-86.6353|   AL|America/Chicago| true|   35019|\n",
      "|     [205]|      Bessemer|     US| Jefferson County|33.4019|-86.9432|   AL|America/Chicago| true|   35020|\n",
      "|     [205]|      Bessemer|     US| Jefferson County|33.4014|-86.9547|   AL|America/Chicago| true|   35021|\n",
      "|     [205]|      Bessemer|     US| Jefferson County|33.3337|-86.9604|   AL|America/Chicago| true|   35022|\n",
      "|     [205]|      Bessemer|     US| Jefferson County|33.4626|-87.0926|   AL|America/Chicago| true|   35023|\n",
      "|     [205]|  Blountsville|     US|    Blount County|34.1179|-86.5621|   AL|America/Chicago| true|   35031|\n",
      "|        []|       Bon Air|     US| Talladega County|33.2638|-86.3359|   AL|America/Chicago| true|   35032|\n",
      "|     [256]|        Bremen|     US|   Cullman County|33.9459|-87.0131|   AL|America/Chicago| true|   35033|\n",
      "|     [205]|         Brent|     US|      Bibb County|32.9234|-87.2762|   AL|America/Chicago| true|   35034|\n",
      "|        []|    Brierfield|     US|      Bibb County|33.0694|-86.9791|   AL|America/Chicago| true|   35035|\n",
      "+----------+--------------+-------+-----------------+-------+--------+-----+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddb29d93-8a40-4523-8f4e-71b7eaa1f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully stored in the database\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# JDBC URL\n",
    "url = f\"jdbc:postgresql://{host}:{port}/{dbname}?currentSchema=schema_pingfanchen23uclacuk\"\n",
    "\n",
    "# Current directory and JDBC driver location\n",
    "current_dir = os.getcwd()\n",
    "jar_location = os.path.join(current_dir, \"postgresql-42.3.2.jar\")\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df_all.write.jdbc(url=url, table=\"zip_code_by_city\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "print(\"Data has been successfully stored in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5f98c-c9b1-4735-acd1-f4bfe23c39ec",
   "metadata": {},
   "source": [
    "### sales tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14043fd1-f428-4a2d-9f8c-866a28214a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "connection_params = {\n",
    "    \"host\":\"13.43.84.151\",\n",
    "    \"user\": \"ping-fan.chen.23@ucl.ac.uk\",\n",
    "    \"password\": \"khsLcR\",\n",
    "    \"dbname\": \"postgres\",\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**connection_params)\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT zip_code FROM schema_pingfanchen23uclacuk.zip_code_by_city;\")\n",
    "df = cur.fetchall()\n",
    "zip_list = [i[0] for i in df]\n",
    "print(zip_list[:10])\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10912aa8-1db9-4b7c-aacf-cfecd49d0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "df_list = []\n",
    "for zip_code in zip_list[::100]:\n",
    "    api_url = 'https://api.api-ninjas.com/v1/salestax?zip_code={}'.format(zip_code)\n",
    "    response = requests.get(api_url, headers={'X-Api-Key': '0+WwzqaBAr5kWjywXArIpA==DlpdDtBfCV9Pb7Nw'})\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        data = response.json()\n",
    "        df = spark.createDataFrame(data)\n",
    "        df_list.append(df)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "df_all = reduce(DataFrame.unionAll, df_list)\n",
    "# Show the dataframe\n",
    "df_all.show()\n",
    "\n",
    "# JDBC URL\n",
    "url = f\"jdbc:postgresql://{host}:{port}/{dbname}?currentSchema=schema_pingfanchen23uclacuk\"\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "df_all.write.jdbc(url=url, table=\"sale_tax_by_zip\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "print(\"Data has been successfully stored in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7514d7e-d5c0-46c5-ba96-d949a83862be",
   "metadata": {},
   "source": [
    "### Median Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ffb92-d2d1-440b-8423-1fd2778a3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_table(column,df):\n",
    "    table = df[column].reset_index()\n",
    "    city,state = column[0].split(',')\n",
    "    table['State'] = state\n",
    "    table['City']=city\n",
    "    table['Price'] = table[column]\n",
    "    table['Time'] = table['index']\n",
    "    table = table[['State','City','Time','Price']]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4427e25-3e2d-447d-ab53-26fa507ae3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_median = pd.read_csv('median_rental_price.csv',skiprows=0)\n",
    "df_median = df_median.loc[1:,df_median.columns[~df_median.columns.isin(['RegionID','SizeRank','RegionType'])]]\n",
    "# df_median.groupby('StateName', group_keys=True)[df_median.columns].apply(lambda x: x)\n",
    "df = pd.DataFrame(df_median[df_median.columns[2:]].values.T,index = df_median.columns[2:],columns= [df_median['RegionName'].values])\n",
    "df.reset_index(names='Time').head()\n",
    "lst = []\n",
    "for column in df.columns[1:]:\n",
    "    lst.append(reshape_table(column,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d653cf-d1d9-4a07-bd8f-e13e00106965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(lst)\n",
    "# df.columns = ['State','City','Time','Price']\n",
    "df['State'].astype('str')\n",
    "df['City'].astype('str')\n",
    "df.dropna(subset = ['Price'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0686d7a-4ad5-4d74-a1a7-82b2d256690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.to_csv('median_rent_price.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2138dae-a5a0-41cd-87a3-c4f5f9f86b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/08 17:05:19 INFO InMemoryFileIndex: It took 91 ms to list leaf files for 1 paths.\n",
      "24/03/08 17:05:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "24/03/08 17:05:22 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/08 17:05:22 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "24/03/08 17:05:23 INFO CodeGenerator: Code generated in 400.774784 ms\n",
      "24/03/08 17:05:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.0 KiB, free 413.7 MiB)\n",
      "24/03/08 17:05:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 413.7 MiB)\n",
      "24/03/08 17:05:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 (size: 34.5 KiB, free: 413.9 MiB)\n",
      "24/03/08 17:05:23 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/08 17:05:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7725389 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/08 17:05:23 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/08 17:05:23 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/08 17:05:23 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/03/08 17:05:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/08 17:05:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/08 17:05:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/08 17:05:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 413.7 MiB)\n",
      "24/03/08 17:05:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 413.7 MiB)\n",
      "24/03/08 17:05:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 (size: 6.4 KiB, free: 413.9 MiB)\n",
      "24/03/08 17:05:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/08 17:05:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/08 17:05:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/03/08 17:05:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk, executor driver, partition 0, PROCESS_LOCAL, 8232 bytes) \n",
      "24/03/08 17:05:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/03/08 17:05:24 INFO CodeGenerator: Code generated in 13.621742 ms\n",
      "24/03/08 17:05:24 INFO FileScanRDD: Reading File path: file:///home/jovyan/dataengineer/median_rent_price.csv, range: 0-3531085, partition values: [empty row]\n",
      "24/03/08 17:05:24 INFO CodeGenerator: Code generated in 63.603325 ms(0 + 1) / 1]\n",
      "24/03/08 17:05:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1636 bytes result sent to driver\n",
      "24/03/08 17:05:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 473 ms on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk (executor driver) (1/1)\n",
      "24/03/08 17:05:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/08 17:05:24 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.887 s\n",
      "24/03/08 17:05:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/08 17:05:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/08 17:05:24 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.000071 s\n",
      "24/03/08 17:05:24 INFO CodeGenerator: Code generated in 14.607796 ms            \n",
      "24/03/08 17:05:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 in memory (size: 6.4 KiB, free: 413.9 MiB)\n",
      "24/03/08 17:05:24 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/08 17:05:24 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/08 17:05:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.0 KiB, free 413.5 MiB)\n",
      "24/03/08 17:05:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 413.5 MiB)\n",
      "24/03/08 17:05:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 (size: 34.5 KiB, free: 413.9 MiB)\n",
      "24/03/08 17:05:25 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/08 17:05:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7725389 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/08 17:05:25 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/08 17:05:25 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/08 17:05:25 INFO CodeGenerator: Code generated in 14.257878 ms\n",
      "24/03/08 17:05:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 199.9 KiB, free 413.3 MiB)\n",
      "24/03/08 17:05:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 413.2 MiB)\n",
      "24/03/08 17:05:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 (size: 34.5 KiB, free: 413.8 MiB)\n",
      "24/03/08 17:05:25 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/08 17:05:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7725389 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/08 17:05:25 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/08 17:05:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.2 KiB, free 413.2 MiB)\n",
      "24/03/08 17:05:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 413.2 MiB)\n",
      "24/03/08 17:05:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 (size: 7.6 KiB, free: 413.8 MiB)\n",
      "24/03/08 17:05:25 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/08 17:05:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/03/08 17:05:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk, executor driver, partition 0, PROCESS_LOCAL, 8232 bytes) \n",
      "24/03/08 17:05:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/03/08 17:05:25 INFO CodeGenerator: Code generated in 15.594564 ms\n",
      "24/03/08 17:05:25 INFO FileScanRDD: Reading File path: file:///home/jovyan/dataengineer/median_rent_price.csv, range: 0-3531085, partition values: [empty row]\n",
      "24/03/08 17:05:25 INFO CodeGenerator: Code generated in 71.427638 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+--------+\n",
      "|State|       City|      Time|   Price|\n",
      "+-----+-----------+----------+--------+\n",
      "|   CA|Los Angeles|2008-02-29|470000.0|\n",
      "|   CA|Los Angeles|2008-03-31|456000.0|\n",
      "|   CA|Los Angeles|2008-04-30|457000.0|\n",
      "|   CA|Los Angeles|2008-05-31|440000.0|\n",
      "|   CA|Los Angeles|2008-06-30|435000.0|\n",
      "+-----+-----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/08 17:05:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1645 bytes result sent to driver\n",
      "24/03/08 17:05:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 191 ms on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk (executor driver) (1/1)\n",
      "24/03/08 17:05:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/08 17:05:25 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.204 s\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/08 17:05:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/03/08 17:05:25 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.209076 s\n",
      "24/03/08 17:05:25 INFO CodeGenerator: Code generated in 11.493138 ms\n",
      "24/03/08 17:08:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 in memory (size: 7.6 KiB, free: 413.8 MiB)\n",
      "24/03/08 17:08:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on jupyter-ping-2dfan-2echen-2e23-40ucl-2eac-2euk:44103 in memory (size: 34.5 KiB, free: 413.9 MiB)\n"
     ]
    }
   ],
   "source": [
    "median_price = spark.read.option(\"header\", \"true\").csv('median_rent_price.csv')\n",
    "median_price.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466b9ce-37f7-4b25-bbf1-65188e57a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDBC URL\n",
    "url = f\"jdbc:postgresql://{host}:{port}/{dbname}?currentSchema=schema_pingfanchen23uclacuk\"\n",
    "\n",
    "# Current directory and JDBC driver location\n",
    "current_dir = os.getcwd()\n",
    "jar_location = os.path.join(current_dir, \"postgresql-42.3.2.jar\")\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "median_price.write.jdbc(url=url, table=\"median_price\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "print(\"Data has been successfully stored in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed44cc-8c1c-46b6-8cce-b317dd8115c7",
   "metadata": {},
   "source": [
    "### Check data is in the right format of database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b92de2-bc33-4857-a240-cfa970675b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available schemas:\n",
      "pg_catalog\n",
      "information_schema\n",
      "schema_pingfanchen23uclacuk\n",
      "public\n"
     ]
    }
   ],
   "source": [
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query to get all schemas\n",
    "cur.execute(\"SELECT schema_name FROM information_schema.schemata;\")\n",
    "schemas = cur.fetchall()\n",
    "\n",
    "# Print the available schemas\n",
    "print(\"Available schemas:\")\n",
    "for schema in schemas:\n",
    "    print(schema[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e02f7e0-09f3-4233-a1f7-b016d03a0407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in schema 'schema_pingfanchen23uclacuk':\n",
      "zip_code_by_city\n",
      "sale_tax_by_zip\n",
      "median_price\n"
     ]
    }
   ],
   "source": [
    "# Query to get all table names in a specific schema\n",
    "cur = conn.cursor()\n",
    "schema_to_check = 'schema_pingfanchen23uclacuk'  # replace with your actual schema name\n",
    "cur.execute(f\"SELECT table_name FROM information_schema.tables WHERE table_schema = '{schema_to_check}';\")\n",
    "tables = cur.fetchall()\n",
    "\n",
    "# Print the tables in the schema\n",
    "print(f\"Tables in schema '{schema_to_check}':\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34b21c93-645c-4286-a355-f08e2c97eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table: zip_code_by_city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75/3626738777.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  area_codes            city country             county      lat       lon  \\\n",
      "0      [205]           Moody      US   St. Clair County  33.6034  -86.4944   \n",
      "1      [205]      Adamsville      US   Jefferson County  33.5929  -86.9940   \n",
      "2      [205]           Adger      US   Jefferson County  33.4462  -87.2230   \n",
      "3      [205]       Alabaster      US      Shelby County  33.2187  -86.7835   \n",
      "4      [256]  Alexander City      US  Tallapoosa County  32.9011  -85.9178   \n",
      "\n",
      "  state         timezone  valid zip_code  \n",
      "0    AL  America/Chicago   True    35004  \n",
      "1    AL  America/Chicago   True    35005  \n",
      "2    AL  America/Chicago   True    35006  \n",
      "3    AL  America/Chicago   True    35007  \n",
      "4    AL  America/Chicago   True    35010  \n",
      "Data from table: sale_tax_by_zip\n",
      "  additional_rate city_rate county_rate state_rate total_rate zip_code\n",
      "0        0.020000  0.040000    0.000000   0.040000   0.100000    35004\n",
      "1        0.020000  0.040000    0.000000   0.040000   0.100000    35173\n",
      "2        0.000000  0.030000    0.030000   0.040000   0.100000    35444\n",
      "3        0.000000  0.045000    0.010000   0.040000   0.095000    35634\n",
      "4        0.000000  0.040000    0.010000   0.040000   0.090000    35962\n",
      "Data from table: median_price\n",
      "  RegionID SizeRank       RegionName RegionType StateName 2008-02-29  \\\n",
      "0   102001        0    United States    country      None   174000.0   \n",
      "1   394913        1     New York, NY        msa        NY   400000.0   \n",
      "2   753899        2  Los Angeles, CA        msa        CA   470000.0   \n",
      "3   394463        3      Chicago, IL        msa        IL   224000.0   \n",
      "4   394810      348    Lumberton, NC        msa        NC       None   \n",
      "\n",
      "  2008-03-31 2008-04-30 2008-05-31 2008-06-30  ... 2023-03-31 2023-04-30  \\\n",
      "0   179000.0   180000.0   180700.0   186000.0  ...   325000.0   334900.0   \n",
      "1   390000.0   395000.0   395000.0   400000.0  ...   500000.0   540000.0   \n",
      "2   456000.0   457000.0   440000.0   435000.0  ...   845000.0   850000.0   \n",
      "3   230000.0   228000.0   235000.0   242000.0  ...   284900.0   295000.0   \n",
      "4       None       None       None       None  ...   120000.0   115000.0   \n",
      "\n",
      "  2023-05-31 2023-06-30 2023-07-31 2023-08-31 2023-09-30 2023-10-31  \\\n",
      "0   345000.0   350000.0   349900.0   345000.0   335000.0   330000.0   \n",
      "1   550000.0   572000.0   585000.0   600000.0   576000.0   560000.0   \n",
      "2   860000.0   885000.0   890000.0   895000.0   885000.0   890500.0   \n",
      "3   310000.0   322000.0   315000.0   310000.0   300000.0   289000.0   \n",
      "4   104000.0   129950.0   152000.0   131500.0   161500.0   115000.0   \n",
      "\n",
      "  2023-11-30 2023-12-31  \n",
      "0   326000.0   325000.0  \n",
      "1   565000.0   570000.0  \n",
      "2   900000.0   875000.0  \n",
      "3   289000.0   285000.0  \n",
      "4   108000.0   121500.0  \n",
      "\n",
      "[5 rows x 196 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75/3626738777.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "/tmp/ipykernel_75/3626738777.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "conn = psycopg2.connect (\n",
    "    host=host,\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port=port\n",
    ")\n",
    "cur = conn.cursor()\n",
    "# Perform the SQL join query using state as the key, combining the upper,median,lower quartile values together \n",
    "\n",
    "schema = 'schema_pingfanchen23uclacuk'\n",
    "# Execute the query\n",
    "cur.execute(f\"\"\"\n",
    "SELECT table_name FROM information_schema.tables\n",
    "WHERE table_schema = '{schema}'\"\"\")\n",
    "\n",
    "tables = cur.fetchall()\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    print(f\"Data from table: {table_name}\")\n",
    "    \n",
    "    # Fetch data from each table\n",
    "    query = f\"SELECT * FROM {schema}.{table_name};\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(df.head())\n",
    "cur.close()\n",
    "conn.close()\n",
    "# Write the cleaned quartile value DataFrame to PostgreSQL\n",
    "# df.write.jdbc(url=url, table=\"combined_value_quartile\", mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5bc370-9be7-4435-a490-d9e921eca04a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ai8-sym": {
   "notebook_id": "d74e9caa-8403-4458-8659-ddbdddf43dbb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
